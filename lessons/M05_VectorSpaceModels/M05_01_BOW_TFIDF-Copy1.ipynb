{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M05 Homework\n",
    "\n",
    "```yaml\n",
    "course:   DS 5001 \n",
    "topic:    BOW and TFIDF\n",
    "author:   JiHo Lee (qxz6hb)\n",
    "```\n",
    "\n",
    "### Question 1. Show the function you created.\n",
    "\n",
    "> The codes are below.\n",
    "\n",
    "### Question 2. What are the top 20 words in the corpus by TFIDF mean using the `max` count method and `book` as the bag?\n",
    "\n",
    "> elinor       <br>\n",
    "pierre        <br>\n",
    "vernon        <br>\n",
    "marianne      <br>\n",
    "emma          <br>\n",
    "darcy         <br>\n",
    "reginald     <br>\n",
    "babbalanja    <br>\n",
    "catherine     <br>\n",
    "frederica     <br>\n",
    "crawford      <br>\n",
    "fanny         <br>\n",
    "elliot        <br>\n",
    "weston        <br>\n",
    "media         <br>\n",
    "israel        <br>\n",
    "knightley     <br>\n",
    "tilney        <br>\n",
    "elton         <br>\n",
    "bingley       <br>\n",
    "\n",
    "### Question 3. What are the top 20 words in the corpus by TFIDF mean, if you using the `sum` count method and `chapter` as the bag? \n",
    "\n",
    "> her             <br>\n",
    "she             <br>\n",
    "cosmopolitan    <br>\n",
    "pierre          <br>\n",
    "communion      <br>\n",
    "i               <br>\n",
    "sailors         <br>\n",
    "you             <br>\n",
    "hypothetical    <br>\n",
    "mr              <br>\n",
    "and             <br>\n",
    "confidential    <br>\n",
    "the             <br>\n",
    "dream           <br>\n",
    "boon            <br>\n",
    "mrs             <br>\n",
    "elephants       <br>\n",
    "whale           <br>\n",
    "thou            <br>\n",
    "acquaintance    <br>\n",
    "\n",
    "### Question 4. Characterize the general difference between the words in Question 3 and those in Question 2 in terms of part-of-speech.\n",
    "\n",
    "> The top 20 words in Question 2 were calculated using TF-IDF with max at book level. This means that it is likely to have unique or significant terms that relevant in specific contexts. Therefore, these words mostly consist of nameas, nouns, and terms related to the theme of the book.\n",
    "<br><br>\n",
    "The top 20 words in Question 3 wre calculated using TF-IDF with sum at chapter level. Thus, they are likely to include verbs, adjectives, or commonly used terms within the chapter across the books. Thus, the top 20 words generally represent terms frequently used throughout the book. Therefore, you can se e these are consist of her, i, you etc.\n",
    "\n",
    "\n",
    "### Question 5. Compute mean `TFIDF` for vocabularies conditioned on individual author, using *chapter* as the bag and `max` as the `TF` count method. Among the two authors, whose work has the most significant adjective?\n",
    "\n",
    "> Jane Austen <br>\n",
    "_thy_, and _dear_ had the most significant TFIDF values with 0.034276, 0.058711 from individual author, _Herman Melville_ and _Jane Austen_ respectively. Since _dear_ has the most significant value, _Jane Austen_'s work has the most significant adjective.\n",
    "\n",
    "\n",
    "```The code related to questions and answers with explanation is described in each corresponding section below.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up\n",
    "### Import, config, and import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly_express as px\n",
    "\n",
    "sns.set()\n",
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../../../env.ini\")\n",
    "data_home = config['DEFAULT']['data_home'] \n",
    "output_dir = config['DEFAULT']['output_dir']\n",
    "data_prefix = 'austen-melville'\n",
    "\n",
    "\n",
    "data_home = data_home.replace('/', '\\\\')\n",
    "output_dir = output_dir.replace('/', '\\\\')\n",
    "\n",
    "OHCO = ['book_id', 'chap_id', 'para_num', 'sent_num', 'token_num']\n",
    "bags = dict(\n",
    "    SENTS = OHCO[:4],\n",
    "    PARAS = OHCO[:3],\n",
    "    CHAPS = OHCO[:2],\n",
    "    BOOKS = OHCO[:1]\n",
    ")\n",
    "\n",
    "LIB = pd.read_csv(f\"{output_dir}\\\\{data_prefix}-LIB.csv\").set_index('book_id')\n",
    "TOKEN = pd.read_csv(f'{output_dir}\\\\{data_prefix}-CORPUS.csv').set_index(OHCO).dropna()\n",
    "# VOCAB = pd.read_csv(f'{output_dir}\\\\{data_prefix}-VOCAB.csv').set_index('term_str').dropna()\n",
    "# POS_GROUP = pd.read_csv(f'{output_dir}/{data_prefix}-POS_GROUP.csv').set_index('pos_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Question 1.</mark>\n",
    "\n",
    "The function to generate a BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_bag(TOKEN, ohco_level):\n",
    "    \n",
    "    bag_tmp = ohco_level\n",
    "    ans = TOKEN.groupby(bags[bag_tmp]+['term_str']).term_str.count().to_frame('n') \n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Question 1.</mark>\n",
    "\n",
    "The function that returns the TFIDF values for a given BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_tfidf(BOW, TF_type):\n",
    "#     log2(N/DF)\n",
    "#     N : the number of documents (aka 'bags') in the BOW\n",
    "\n",
    "    DTCM = BOW.n.unstack(fill_value=0)\n",
    "    method = TF_type\n",
    "    print('TF method:', method)\n",
    "\n",
    "    if method == 'sum':\n",
    "        TF = DTCM.T / DTCM.T.sum()\n",
    "    elif method == 'max':\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "    elif method == 'log':\n",
    "        TF = np.log2(1 + DTCM.T)\n",
    "    elif method == 'raw':\n",
    "        TF = DTCM.T\n",
    "    elif method == 'double_norm':\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "    elif method == 'binary':\n",
    "        TF = DTCM.T.astype('bool').astype('int')\n",
    "    TF = TF.T\n",
    "    \n",
    "    DF = DTCM.astype('bool').sum() \n",
    "    N = DTCM.shape[0]\n",
    "    \n",
    "    idf_method = 'standard'   # standard, max, smooth\n",
    "    if idf_method == 'standard':\n",
    "        IDF = np.log2(N / DF)\n",
    "    elif idf_method == 'max':\n",
    "        IDF = np.log2(DF.max() / DF) \n",
    "    elif idf_method == 'smooth':\n",
    "        IDF = np.log2((1 + N) / (1 + DF)) + 1\n",
    "    \n",
    "    TFIDF = TF * IDF\n",
    "    \n",
    "    return TFIDF\n",
    "# BOW['tf'] = TF.stack()\n",
    "# BOW['tfidf'] = TFIDF.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Question 2.</mark>\n",
    "Top 20 words and their TFIDF values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF method: max\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "term_str\n",
       "elinor        0.033840\n",
       "pierre        0.030911\n",
       "vernon        0.025980\n",
       "marianne      0.021347\n",
       "emma          0.021164\n",
       "darcy         0.019302\n",
       "reginald      0.018486\n",
       "babbalanja    0.018252\n",
       "catherine     0.018238\n",
       "frederica     0.017986\n",
       "crawford      0.017749\n",
       "fanny         0.017167\n",
       "elliot        0.017053\n",
       "weston        0.016591\n",
       "media         0.015986\n",
       "israel        0.015428\n",
       "knightley     0.015184\n",
       "tilney        0.013815\n",
       "elton         0.013648\n",
       "bingley       0.013264\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = gen_bag(TOKEN, 'BOOKS')\n",
    "ans.head(10)\n",
    "\n",
    "ans_TFIDF = cal_tfidf(ans, 'max')\n",
    "\n",
    "ans_TFIDF.mean(axis=0).sort_values(ascending=False)[:20]\n",
    "# ans_TFIDF.sort_values('pkr').head(20).style.background_gradient(cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Question 3.</mark>\n",
    "Top 20 words and their TFIDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF method: sum\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "term_str\n",
       "her             0.004327\n",
       "she             0.004150\n",
       "cosmopolitan    0.003485\n",
       "pierre          0.003317\n",
       "communion       0.003004\n",
       "i               0.002771\n",
       "sailors         0.002668\n",
       "you             0.002620\n",
       "hypothetical    0.002437\n",
       "mr              0.002084\n",
       "and             0.002054\n",
       "confidential    0.002042\n",
       "the             0.001972\n",
       "dream           0.001942\n",
       "boon            0.001857\n",
       "mrs             0.001747\n",
       "elephants       0.001731\n",
       "whale           0.001715\n",
       "thou            0.001696\n",
       "acquaintance    0.001690\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = gen_bag(TOKEN, 'CHAPS')\n",
    "\n",
    "ans_TFIDF = cal_tfidf(ans, 'sum')\n",
    "# ans_TFIDF.head()\n",
    "ans_TFIDF.mean(axis=0).sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Question 5.</mark>\n",
    "Jane Austen <br>\n",
    "_thy_, and _dear_ had the most significant TFIDF values with 0.034276, 0.058711 from individual author, _Herman Melville_ and _Jane Austen_ respectively. Since _dear_ has the most significant value, _Jane Austen_'s work has the most significant adjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF method: max\n",
      "term_str\n",
      "thy    0.034276\n",
      "dtype: float64\n",
      "\n",
      "term_str\n",
      "dear    0.058711\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "VOCAB = TOKEN.term_str.value_counts().to_frame('n').sort_index()\n",
    "VOCAB.index.name = 'term_str'\n",
    "VOCAB['n_chars'] = VOCAB.index.str.len()\n",
    "VOCAB['p'] = VOCAB.n / VOCAB.n.sum()\n",
    "VOCAB['i'] = -np.log2(VOCAB.p)\n",
    "VOCAB['max_pos'] = TOKEN[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)\n",
    "VOCAB['max_pos_group'] = TOKEN[['term_str','pos_group']].value_counts().unstack(fill_value=0).idxmax(1)\n",
    "\n",
    "ans = TOKEN.groupby(bags['CHAPS']+['term_str']).term_str.count().to_frame('n') \n",
    "ans = ans.join(LIB[['author', 'title']])\n",
    "# ans['author'] = ans.set_index('author')\n",
    "\n",
    "ans_TFIDF = cal_tfidf(ans, 'max')\n",
    "ans_TFIDF =  ans_TFIDF.join(LIB[['author']], how = 'left', lsuffix='', rsuffix='right')\n",
    "tmp = ans_TFIDF.groupby('authorright').mean()\n",
    "\n",
    "tmp.columns.name = 'term_str'\n",
    "adjectives = VOCAB[VOCAB['max_pos'].isin(['JJ', 'JJR', 'JJS'])].index\n",
    "tmp_adj = tmp.loc[:, tmp.columns.intersection(adjectives)]\n",
    "tmp_adj.columns.name = 'term_str'\n",
    "\n",
    "print(tmp_adj[tmp_adj.index=='MELVILLE, HERMAN'].mean(axis=0).sort_values(ascending=False)[:1])\n",
    "print()\n",
    "print(tmp_adj[tmp_adj.index=='AUSTEN, JANE'].mean(axis=0).sort_values(ascending=False)[:1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "235.517px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

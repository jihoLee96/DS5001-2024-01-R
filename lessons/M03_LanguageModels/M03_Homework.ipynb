{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6084ed44-0d7d-4833-a10a-1aac3f73cb13",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# M03 Homework\n",
    "\n",
    "```yaml\n",
    "Course:  DS 5001\n",
    "Module:  M03 Language Models\n",
    "Author:  JiHo Lee (qxz6hb)\n",
    "```\n",
    "\n",
    "### Question 1. List six words that precede the word \"monster,\" excluding stop words (and sentence boundary markers). Stop words include 'a', 'an', 'the', 'this', 'that', etc. Hint: use the df.query() method.\n",
    "\n",
    "> miserable, abhorred, detestable, hideous, hellish, gigantic\n",
    "\n",
    "### Question 2. List the following sentences in ascending order of bigram perplexity according to the language model generated from the text:\n",
    "\n",
    "> I have never seen the aurora borealis. <br> The monster is on the ice.  <br>\n",
    "> He never knew the love of a family. <br>\n",
    "> Flowers are happy things\n",
    "\n",
    "### Question 3. Using the bigram model represented as a matrix, explore the relationship between bigram pairs using the following lists. H\n",
    "\n",
    "> he   said     5.062035<br>he   heard    7.130229  \n",
    "<br>she  said     6.771566\n",
    "<br>she  heard    6.771566     \n",
    "\n",
    "<b>''he said''</b> is the most likely bigram to occur in the corpus. <b>''he heard''</b> is the least likely bigram in the corpus. <b>''she said'', ''she heard''</b> have equal likelihoods.\n",
    "\n",
    "\n",
    "### Question 4. Generate 20 sentences using the generate_text() function. Display the results.\n",
    "\n",
    "> ```The generated sentences are described below in the according section with the code```\n",
    "\n",
    "\n",
    "### Question 5. Compute the redundancy  for each of the n-gram models using the MLE of the joint probability of each ngram type. \n",
    "\n",
    "> Each redundancy for unigram, bigram, trigram is <b>0.309, 0.112, 0.0684</b> respectively.\n",
    "<br> The redundancy _R_ decreases as the length of the n-gram increases.\n",
    "\n",
    "```The code related to questions and answers with explanation is described in each\n",
    "corresponding section below.```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093194b-7f8f-439d-89f0-20ada46cb636",
   "metadata": {},
   "source": [
    "### Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a30beeb-1f4a-46bb-b42a-c637ae0d43ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../../../env.ini\")\n",
    "data_dir = config['DEFAULT']['data_home']\n",
    "output_dir = config['DEFAULT']['output_dir']\n",
    "\n",
    "data_dir  = data_dir.replace('/', '\\\\')\n",
    "output_dir  = output_dir.replace('/', '\\\\')\n",
    "\n",
    "ngram_order = 3\n",
    "widx = [f\"w{i}\" for i in range(ngram_order)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b8ac85f-7751-4b83-9232-2a9c0f4a7473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing  C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\pg42324.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^(?:INTRODUCTION|PREFACE|LETTER|CHAPTER)\\b\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by delimitter [.?!;:]+\n",
      "Parsing OHCO level 3 token_num by delimitter [\\s',-]+\n"
     ]
    }
   ],
   "source": [
    "# data_dir = output_dir\n",
    "OHCO = ['book_id','chap_id','para_num','sent_num','token_num']\n",
    "\n",
    "text_file = f\"{data_dir}/gutenberg/pg42324.txt\"\n",
    "csv_token  = f\"{output_dir}/austen-combo-TOKENS.csv\" # The file we will create\n",
    "csv_vocab = f\"{output_dir}/austen-combo-VOCAB.csv\" \n",
    "\n",
    "\n",
    "text_file = text_file.replace('/', '\\\\')\n",
    "csv_token = csv_token.replace('/', '\\\\')\n",
    "csv_vocab = csv_vocab.replace('/', '\\\\')\n",
    "\n",
    "from textimporter import TextImporter\n",
    "from textparser import TextParser\n",
    "\n",
    "src_file = text_file\n",
    "# ohco_pats = [('chap', r'^(?:INTRODUCTION|PREFACE|LETTER|CHAPTER)\\.?\\b', 'm')]\n",
    "ohco_pats = [('chap', r'^(?:INTRODUCTION|PREFACE|LETTER|CHAPTER)\\b', 'm')]\n",
    "clip_pats = [r'START', r'END']\n",
    "\n",
    "test= TextImporter(src_file=src_file, ohco_pats=ohco_pats, clip_pats=clip_pats)\n",
    "test.import_source().parse_tokens()\n",
    "\n",
    "TOKEN = test.TOKENS\n",
    "test.extract_vocab()\n",
    "VOCAB = test.VOCAB\n",
    "\n",
    "VOCAB['n_chars'] = VOCAB.index.str.len()\n",
    "VOCAB['modified_term_str'] = VOCAB.index\n",
    "VOCAB.loc[(VOCAB.n == 1) & (VOCAB.n_chars < 3), 'modified_term_str'] = \"<UNK>\"\n",
    "\n",
    "TOKEN['modified_term_str'] = TOKEN.term_str.map(VOCAB.modified_term_str)\n",
    "# TOKEN[TOKEN.modified_term_str == '<UNK>'].sample(5)\n",
    "\n",
    "def token_to_padded(token, grouper=['sent_num'], term_str='term_str'):\n",
    "    ohco = token.index.names # We preserve these since they get lost in the shuffle'\n",
    "    padded = token.groupby(grouper)\\\n",
    "        .apply(lambda x: '<s> ' + ' '.join(x[term_str]) + ' </s>')\\\n",
    "        .apply(lambda x: pd.Series(x.split()))\\\n",
    "        .stack().to_frame('term_str')\n",
    "    padded.index.names = ohco\n",
    "    return padded\n",
    "def padded_to_ngrams(padded, grouper=['sent_num'], n=2):\n",
    "\n",
    "    ohco = padded.index.names # This gets lost in the grouping\n",
    "    G = padded.groupby(grouper)\n",
    "    ng = [None for i in range(ngram_order)]\n",
    "    for i in range(ngram_order):\n",
    "        if i == 0:\n",
    "            ng[0] = G.apply(lambda x: x)\n",
    "        else:\n",
    "            ng[i] = G.apply(lambda x: pd.concat([x.shift(i) for i in range(i,-1,-1)], axis=1))\n",
    "        ng[i].columns = widx[:i+1]    \n",
    "        ng[i] = ng[i].reset_index(drop=True)\n",
    "        ng[i].index = padded.index\n",
    "        ng[i] = ng[i].fillna('</s>')\n",
    "            \n",
    "    return ng\n",
    "PADDED = token_to_padded(TOKEN, grouper=OHCO[1:4], term_str='modified_term_str')\n",
    "PADDED = PADDED.unstack().reset_index(drop=True).stack().dropna()\n",
    "PADDED.index.names = ['sent_num', 'token_num']\n",
    "\n",
    "VOCAB2 = PADDED.term_str.value_counts().to_frame('n')\n",
    "NG = padded_to_ngrams(PADDED, n=ngram_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d91c047",
   "metadata": {},
   "source": [
    "### <mark>Question 1.</mark>\n",
    "miserable, abhorred, detestable, hideous, hellish, gigantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14341241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>w0</th>\n",
       "      <th>w1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <th>26</th>\n",
       "      <td>miserable</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2233</th>\n",
       "      <th>2</th>\n",
       "      <td>abhorred</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3035</th>\n",
       "      <th>24</th>\n",
       "      <td>detestable</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3254</th>\n",
       "      <th>2</th>\n",
       "      <td>hideous</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4719</th>\n",
       "      <th>6</th>\n",
       "      <td>hellish</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4817</th>\n",
       "      <th>3</th>\n",
       "      <td>gigantic</td>\n",
       "      <td>monster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            w0       w1\n",
       "sent_num token_num                     \n",
       "1111     26          miserable  monster\n",
       "2233     2            abhorred  monster\n",
       "3035     24         detestable  monster\n",
       "3254     2             hideous  monster\n",
       "4719     6             hellish  monster\n",
       "4817     3            gigantic  monster"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = ['this', 'the', 'a', '<s>']\n",
    "tmp = NG[1].query(\"w1=='monster'\")\n",
    "tmp2 = tmp[~tmp['w0'].isin(['this', 'the', 'a', '<s>'])]\n",
    "tmp2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c87c66a7-39f9-4a03-9af5-cd4942aaab41",
   "metadata": {},
   "source": [
    "### Generate NGram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95411db1-627c-4314-8ca6-012edeec8603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ngrams_to_models(ngrams, k=.01):\n",
    "\n",
    "    model = [None for i in range(ngram_order)]\n",
    "    K = len(VOCAB2) * k\n",
    "    \n",
    "    for i in range(ngram_order):    \n",
    "        if i == 0:\n",
    "            model[i] = ngrams[i].value_counts().to_frame('n')\n",
    "            model[i]['p'] = (model[i].n + k) / (model[i].n.sum() + K)\n",
    "            model[i]['i'] = np.log2(1/model[i].p)\n",
    "        else:\n",
    "            model[i] = ngrams[i].value_counts().to_frame('n')\n",
    "            model[i]['cp'] = (model[i].n + k) / (model[i-1].n + K)\n",
    "            model[i]['i'] = np.log2(1/model[i].cp)\n",
    "        model[i] = model[i].sort_index()\n",
    "            \n",
    "    return model\n",
    "M = ngrams_to_models(NG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d10ee8-af3e-493b-851c-f7729e327ff5",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3737ce9d-7f08-45cd-a3d9-d088a697bf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ngrams_to_matrix_models(ngrams, k=.01):\n",
    "    M = [None for i in range(ngram_order)]\n",
    "    for i in range(ngram_order):\n",
    "        M[i] = NG[i].value_counts()\n",
    "        if i > 0:\n",
    "            M[i] = M[i].unstack(fill_value=0).T + k\n",
    "        else:\n",
    "            M[i] = M[i].to_frame('i')\n",
    "        M[i] = M[i] / M[i].sum()\n",
    "        M[i] = -np.log2(M[i])        \n",
    "    return M\n",
    "\n",
    "S= \"The monster is on the ice.\\nFlowers are happy things.\\nI have never seen the aurora borealis.\\nHe never knew the love of a family.\"\n",
    "S = S.split('\\n')\n",
    "S = pd.DataFrame(S, columns=['sent_str'])\n",
    "S.index.name='sent_num'\n",
    "\n",
    "# Convert dataframe of sentences to TOKEN with normalized terms\n",
    "K = S.sent_str.apply(lambda x: pd.Series(x.split())).stack().to_frame('token_str')\n",
    "K['term_str'] = K.token_str.str.replace(r\"[\\W_]+\", \"\", regex=True).str.lower()\n",
    "K.index.names = ['sent_num', 'token_num']\n",
    "\n",
    "TEST_SENTS, TEST_TOKENS = S, K\n",
    "\n",
    "TEST_TOKENS.loc[~TEST_TOKENS.term_str.isin(VOCAB2.index), 'term_str'] = \"<UNK>\"\n",
    "\n",
    "TEST_PADDED = token_to_padded(TEST_TOKENS)\n",
    "TEST_NG = padded_to_ngrams(TEST_PADDED, 'sent_num', ngram_order)\n",
    "\n",
    "def test_model(model, ngrams, sents):\n",
    "        \n",
    "    n = len(model)\n",
    "    ohco = ngrams[0].index.names\n",
    "    \n",
    "    max_i = np.log2(len(M[0])) # For unseen combos\n",
    "    \n",
    "    R = []\n",
    "    for i in range(n):\n",
    "\n",
    "        cols = widx[:i+1]\n",
    "            \n",
    "        T = ngrams[i].merge(M[i], on=cols, how='left')\n",
    "        T.index = ngrams[i].index\n",
    "        T = T.reset_index().set_index(ohco + cols).i\n",
    "        \n",
    "        # This how we handle unseen combos\n",
    "        T[T.isna()] = max_i\n",
    "\n",
    "        R.append(T.to_frame(f\"i{i}\"))\n",
    "                \n",
    "    return R\n",
    "\n",
    "R = test_model(M, TEST_NG, TEST_SENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b2bb4-0381-4743-9c61-60b3b796e368",
   "metadata": {},
   "source": [
    "### Sentence Perplexity\n",
    "\n",
    "### <mark>Question 2.</mark>\n",
    "\n",
    "I have never seen the aurora borealis.\n",
    "\n",
    "The monster is on the ice. \n",
    "\n",
    "He never knew the love of a family.\t\n",
    "\n",
    "Flowers are happy things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9edd847a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_53211_row0_col1 {\n",
       "  background-color: #2296c1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_53211_row0_col2 {\n",
       "  background-color: #69c5be;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_53211_row0_col3, #T_53211_row0_col4, #T_53211_row1_col1, #T_53211_row1_col2, #T_53211_row1_col4, #T_53211_row1_col5, #T_53211_row1_col6 {\n",
       "  background-color: #ffffd9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_53211_row0_col5 {\n",
       "  background-color: #dff2b2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_53211_row0_col6 {\n",
       "  background-color: #fafdce;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_53211_row1_col3 {\n",
       "  background-color: #fdfed4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_53211_row2_col1 {\n",
       "  background-color: #c8e9b4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_53211_row2_col2 {\n",
       "  background-color: #edf8b1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_53211_row2_col3 {\n",
       "  background-color: #a9ddb7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_53211_row2_col4 {\n",
       "  background-color: #f1fabb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_53211_row2_col5 {\n",
       "  background-color: #a0dab8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_53211_row2_col6 {\n",
       "  background-color: #f3fabd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_53211_row3_col1, #T_53211_row3_col2, #T_53211_row3_col3, #T_53211_row3_col4, #T_53211_row3_col5, #T_53211_row3_col6 {\n",
       "  background-color: #081d58;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_53211\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_53211_level0_col0\" class=\"col_heading level0 col0\" >sent_str</th>\n",
       "      <th id=\"T_53211_level0_col1\" class=\"col_heading level0 col1\" >mean_i_0</th>\n",
       "      <th id=\"T_53211_level0_col2\" class=\"col_heading level0 col2\" >pp0</th>\n",
       "      <th id=\"T_53211_level0_col3\" class=\"col_heading level0 col3\" >mean_i_1</th>\n",
       "      <th id=\"T_53211_level0_col4\" class=\"col_heading level0 col4\" >pp1</th>\n",
       "      <th id=\"T_53211_level0_col5\" class=\"col_heading level0 col5\" >mean_i_2</th>\n",
       "      <th id=\"T_53211_level0_col6\" class=\"col_heading level0 col6\" >pp2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >sent_num</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_53211_level0_row0\" class=\"row_heading level0 row0\" >2</th>\n",
       "      <td id=\"T_53211_row0_col0\" class=\"data row0 col0\" >I have never seen the aurora borealis.</td>\n",
       "      <td id=\"T_53211_row0_col1\" class=\"data row0 col1\" >8.228776</td>\n",
       "      <td id=\"T_53211_row0_col2\" class=\"data row0 col2\" >299.991202</td>\n",
       "      <td id=\"T_53211_row0_col3\" class=\"data row0 col3\" >5.525197</td>\n",
       "      <td id=\"T_53211_row0_col4\" class=\"data row0 col4\" >46.052165</td>\n",
       "      <td id=\"T_53211_row0_col5\" class=\"data row0 col5\" >9.306330</td>\n",
       "      <td id=\"T_53211_row0_col6\" class=\"data row0 col6\" >633.117678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_53211_level0_row1\" class=\"row_heading level0 row1\" >0</th>\n",
       "      <td id=\"T_53211_row1_col0\" class=\"data row1 col0\" >The monster is on the ice.</td>\n",
       "      <td id=\"T_53211_row1_col1\" class=\"data row1 col1\" >6.870571</td>\n",
       "      <td id=\"T_53211_row1_col2\" class=\"data row1 col2\" >117.016718</td>\n",
       "      <td id=\"T_53211_row1_col3\" class=\"data row1 col3\" >5.593021</td>\n",
       "      <td id=\"T_53211_row1_col4\" class=\"data row1 col4\" >48.268880</td>\n",
       "      <td id=\"T_53211_row1_col5\" class=\"data row1 col5\" >8.573922</td>\n",
       "      <td id=\"T_53211_row1_col6\" class=\"data row1 col6\" >381.072693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_53211_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
       "      <td id=\"T_53211_row2_col0\" class=\"data row2 col0\" >He never knew the love of a family.</td>\n",
       "      <td id=\"T_53211_row2_col1\" class=\"data row2 col1\" >7.426112</td>\n",
       "      <td id=\"T_53211_row2_col2\" class=\"data row2 col2\" >171.981820</td>\n",
       "      <td id=\"T_53211_row2_col3\" class=\"data row2 col3\" >6.687920</td>\n",
       "      <td id=\"T_53211_row2_col4\" class=\"data row2 col4\" >103.101378</td>\n",
       "      <td id=\"T_53211_row2_col5\" class=\"data row2 col5\" >9.930102</td>\n",
       "      <td id=\"T_53211_row2_col6\" class=\"data row2 col6\" >975.570260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_53211_level0_row3\" class=\"row_heading level0 row3\" >1</th>\n",
       "      <td id=\"T_53211_row3_col0\" class=\"data row3 col0\" >Flowers are happy things.</td>\n",
       "      <td id=\"T_53211_row3_col1\" class=\"data row3 col1\" >9.110441</td>\n",
       "      <td id=\"T_53211_row3_col2\" class=\"data row3 col2\" >552.733765</td>\n",
       "      <td id=\"T_53211_row3_col3\" class=\"data row3 col3\" >9.343909</td>\n",
       "      <td id=\"T_53211_row3_col4\" class=\"data row3 col4\" >649.825583</td>\n",
       "      <td id=\"T_53211_row3_col5\" class=\"data row3 col5\" >12.810973</td>\n",
       "      <td id=\"T_53211_row3_col6\" class=\"data row3 col6\" >7186.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x245eb5b5610>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_cols = []\n",
    "mean_i_cols = []\n",
    "for i in range(ngram_order):\n",
    "    pp_col = f\"pp{i}\"\n",
    "    mean_i_col = f\"mean_i_{i}\"\n",
    "    pp_cols.append(pp_col)\n",
    "    mean_i_cols.append(mean_i_col)\n",
    "    TEST_SENTS[mean_i_col] = R[i].groupby('sent_num')[f\"i{i}\"].mean()\n",
    "    TEST_SENTS[f\"pp{i}\"] = np.exp2(TEST_SENTS[mean_i_col])\n",
    "\n",
    "TEST_SENTS.sort_values(\"pp1\").style.background_gradient(cmap=\"YlGnBu\", axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a511b6",
   "metadata": {},
   "source": [
    "### <mark>Question 3.</mark>\n",
    "\n",
    "he   said     5.062035\n",
    "\n",
    "he   heard    7.130229\n",
    "     \n",
    "she  said     6.771566\n",
    "\n",
    "she  heard    6.771566     \n",
    "\n",
    "<b>''he said''</b> is the most likely bigram to occur in the corpus. <b>''he heard''</b> is the least likely bigram in the corpus. <b>''she said'', ''she heard''</b> have equal likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a8d5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "w0   w1   \n",
       "he   said     5.062035\n",
       "     heard    7.130229\n",
       "she  said     6.771566\n",
       "     heard    6.771566\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MM = ngrams_to_matrix_models(NG)\n",
    "\n",
    "tmp = MM[1].unstack()\n",
    "indices = ['he', 'she']\n",
    "columns = ['said', 'heard']\n",
    "\n",
    "relationships = tmp.loc[indices, columns]\n",
    "relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef674cc2-b528-47e6-b0e0-1b1f2ecded3a",
   "metadata": {},
   "source": [
    "### Generate Text\n",
    "\n",
    "### <mark>Question 4.</mark>\n",
    "\n",
    "The generated sentences are in the result below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "389f3c33-3f45-4a71-a195-0cb44591f399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I SHALL PERISH IN THE GREAT AND SUDDEN CHANGE\n",
      "2. HE SAW THE BEAUTIES OF NATURE BUT THAT SHE MADE ANY PROFESSIONS\n",
      "3. VICTOR HE IS NOT MADE OF NO HOPE OR CONSOLATION\n",
      "4. AND I WILL GIVE YOU PAIN\n",
      "5. MY FATHER HAD TAKEN PLACE\n",
      "6. I DARED NOT\n",
      "7. YET YOU WOULD NOT FOREGO THEIR HOLD\n",
      "8. I CANNOT BEAR TO LIVE AND BE OVERWHELMED BY DISAPPOINTMENTS\n",
      "9. I AM ONLY LEFT TO MY OLD HABITS\n",
      "10. AND VERY FAIR\n",
      "11. I THEN WAS IN THE DIFFERENT FEELINGS\n",
      "12. WE ACCORDINGLY LAY TO HOPING THAT SOME ONE SHOULD GO DOWN TO POSTERITY\n",
      "13. A PENETRATION INTO THE SEA OR RATHER THE VAST RIVER OF ICE IS NOT THUS INTERRUPTED THE BEING TO ME THE DUTY OF OBEYING THE DYING REQUEST OF MY HIDEOUS NARRATION\n",
      "14. THE OLD MAN WAS RECOMMENCING HIS MUSIC WHEN SOME ONE TAPPED AT THE ESCAPE OF THEIR VIRTUES AND A DESIRE TO ATTAIN BREATHE A DEGREE OF COMPOSURE\n",
      "15. AND DEPOSITING THE REMAINS OF HIS COMPANION\n",
      "16. HOW ALL THE FIRMNESS OF WHICH WAS TO BE VISITED BY PEACEFUL DREAMS\n",
      "17. IT WAS YOUR JOURNAL OF WHAT HAD HAPPENED\n",
      "18. THE NEXT HOUR MIGHT REALISE\n",
      "19. I CONFESS MY SON WHEN YOU IN ANOTHER FORTNIGHT I WAS STILL SPURNED\n",
      "20. HE REMINDS ME HOW GLADLY I WOULD ACCOUNT TO MYSELF\n",
      "21. \n",
      "22. I THOUGHT OF WHAT HAD PASSED AT THE BOTTOM OF THE GROUND SEA\n",
      "23. \n",
      "24. CEASE\n",
      "25. SO MUCH WITH EVERY AGGRAVATION OF INFAMY THAT COULD NOT DISOBEY\n",
      "26. NOW I AM NOW CONVINCED THAT NO DISASTER\n"
     ]
    }
   ],
   "source": [
    "def generate_text(M, n=290):\n",
    "    \n",
    "    if len(M) < 3:\n",
    "        raise ValueError(\"Must have trigram model generated.\")\n",
    "        \n",
    "    words = ['</s>', '<s>']\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        bg = tuple(words[-2:])\n",
    "\n",
    "        # Try trigram model\n",
    "        try:\n",
    "            next_word = M[2].loc[bg].sample(weights='cp').index[0]\n",
    "\n",
    "        # If not found in model, back off to bigram model\n",
    "        except KeyError as e1:\n",
    "            # print(bg)\n",
    "            try:\n",
    "                # Get the last word in the bigram\n",
    "                ug = bg[1]\n",
    "                next_word = M[1].loc[ug].sample(weights='cp').index[0]\n",
    "            \n",
    "            # ... back off to unigram\n",
    "            except KeyError as e2:\n",
    "                # print(ug)\n",
    "                next_word = M[0].sample(weights='p').index[0][0]\n",
    "        \n",
    "        words.append(next_word)\n",
    "    \n",
    "    text = ' '.join(words[2:])\n",
    "    print('\\n'.join([f\"{i+1}. \" +  line.replace('</s> <s>','').replace('<s>', '').strip().upper() \n",
    "                       for i, line in enumerate(text.split('</s>'))]))\n",
    "generate_text(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60f97e4",
   "metadata": {},
   "source": [
    "### <mark>Question 5.</mark>\n",
    "\n",
    "Each redundancy for unigram, bigram, trigram is <b>0.309, 0.112, 0.0684</b> respectively.\n",
    "\n",
    "The redundancy _R_ decreases as the length of the n-gram increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af5d9e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3091704418633189"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_unigram = -np.sum(M[0]['p'] * np.log2(M[0]['p']))\n",
    "\n",
    "num_unique_unigrams = len(M[0])\n",
    "H_max_unigram = np.log2(num_unique_unigrams)\n",
    "\n",
    "R_unigram = 1 - (H_unigram / H_max_unigram)\n",
    "\n",
    "M_Orig = M\n",
    "R_unigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d047b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11177428801648903"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = M[1]['n'].sum()\n",
    "M[1]['prob'] = M[1]['n'] / total_tokens\n",
    "M[1]['joint_prob'] = M[1]['prob']\n",
    "\n",
    "H_bigram = -np.sum(M[1]['joint_prob'] * np.log2(M[1]['prob']))\n",
    "num_unique_bigrams = len(M[1])\n",
    "H_max_bigram = np.log2(num_unique_bigrams)\n",
    "\n",
    "R_bigram = 1 - (H_bigram / H_max_bigram)\n",
    "\n",
    "R_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "460c9345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06838180312642583"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = M[2]['n'].sum()\n",
    "M[2]['prob'] = M[2]['n'] / total_tokens\n",
    "M[2]['joint_prob'] = M[2]['prob']\n",
    "\n",
    "H_trigram = -np.sum(M[2]['joint_prob'] * np.log2(M[2]['prob']))\n",
    "num_unique_trigrams = len(M[2])\n",
    "H_max_trigram = np.log2(num_unique_trigrams)\n",
    "\n",
    "R_trigram = 1 - (H_trigram / H_max_trigram)\n",
    "\n",
    "R_trigram "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

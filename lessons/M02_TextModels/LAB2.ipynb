{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# M02 Homework\n",
    "\n",
    "```yaml\n",
    "Course:   DS 5001 \n",
    "Author:   JiHo Lee (qxz6hb)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1. How many raw tokens are in the combined data frame?\n",
    "\n",
    "> The number of raw tokens in the combined data frame is <b>207896</b> as below.\n",
    "\n",
    "### Question 2. How many distinct terms are there in the combined data frame (i.e. how big is the vocabulary)?\n",
    "\n",
    "> The number of distinct terms in the combined data frame is <b>8239</b> as below.\n",
    "\n",
    "### Question 3. How many more terms does the vocabulary of Sense and Sensibility have than that of Persuasion?\n",
    "\n",
    "> <b>520</b> more terms.\n",
    "_Sense and Sensibility_ has 6280 terms. _Persuasion_ has 5760 terms.\n",
    "\n",
    "### Question 4. What is the average number of tokens, rounded to an integer, per chapter in the corpus?\n",
    "\n",
    "> The average number of tokens per chapter in the corpus is <b>2809</b>.\n",
    "\n",
    "### Question 5. What is the average number of tokens, rounded to an integer, per paragraph in the corpus?\n",
    "\n",
    "> The average number of tokens per paragraph in the corpus is <b>74</b>.\n",
    "\n",
    "`The code related to questions and answers with explanation is described in each corresponding section below.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../../../env.ini\")\n",
    "data_home = config['DEFAULT']['data_home']\n",
    "output_dir = config['DEFAULT']['output_dir']\n",
    "data_home = data_home.replace('/', '\\\\')\n",
    "output_dir = output_dir.replace('/', '\\\\')\n",
    "\n",
    "text_file = f\"{data_home}/gutenberg/pg161.txt\"\n",
    "csv_file  = f\"{output_dir}/austen-persuasion.csv\" # The file we will create\n",
    "\n",
    "OHCO = ['chap_num', 'para_num', 'sent_num', 'token_num']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINES = pd.DataFrame(open(text_file, 'r', encoding='utf-8-sig').readlines(), columns=['line_str'])\n",
    "LINES.index.name = 'line_num'\n",
    "LINES.line_str = LINES.line_str.str.replace(r'\\n+', ' ', regex=True).str.strip()\n",
    "\n",
    "title = LINES.loc[0].line_str.replace('The Project Gutenberg EBook of ', '')\n",
    "\n",
    "clip_pats = [\n",
    "    r\"\\*\\*\\*\\s*START OF (?:THE|THIS) PROJECT\",\n",
    "    r\"\\*\\*\\*\\s*END OF (?:THE|THIS) PROJECT\"\n",
    "]\n",
    "\n",
    "pat_a = LINES.line_str.str.match(clip_pats[0])\n",
    "pat_b = LINES.line_str.str.match(clip_pats[1])\n",
    "\n",
    "line_a = LINES.loc[pat_a].index[0] + 1\n",
    "line_b = LINES.loc[pat_b].index[0] - 1\n",
    "\n",
    "LINES = LINES.loc[line_a : line_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Chunk by Chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find all chapter headers, Assign numbers to chapters, Forward-fill chapter numbers to following text lines,  Clean up\n",
    "\n",
    "The regex will depend on the source text. You need to investigate the source text to figure this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chap_pat = r\"^\\s*(?:chapter|letter)\\s+\\d+\"\n",
    "chap_lines = LINES.line_str.str.match(chap_pat, case=False) # Returns a truth vector\n",
    "LINES.loc[chap_lines, 'chap_num'] = [i+1 for i in range(LINES.loc[chap_lines].shape[0])]\n",
    "LINES.chap_num = LINES.chap_num.ffill()\n",
    "LINES = LINES.dropna(subset=['chap_num']) # Remove everything before Chapter 1\n",
    "# LINES = LINES.loc[~LINES.chap_num.isna()] # Remove everything before Chapter 1 (alternate method)\n",
    "LINES = LINES.loc[~chap_lines] # Remove chapter heading lines; their work is done\n",
    "LINES.chap_num = LINES.chap_num.astype('int') # Convert chap_num from float to int\n",
    "# Make big string for each chapter\n",
    "CHAPS = LINES.groupby(OHCO[:1])\\\n",
    "    .line_str.apply(lambda x: '\\n'.join(x))\\\n",
    "    .to_frame('chap_str')\n",
    "CHAPS['chap_str'] = CHAPS.chap_str.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split chapters into paragraphs \n",
    "\n",
    "We use Pandas' convenient `.split()` method with `expand=True`, followed by `.stack()`.\n",
    "Note that this creates zero-based indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_pat = r'\\n\\n+'\n",
    "PARAS = CHAPS['chap_str'].str.split(para_pat, expand=True).stack()\\\n",
    "    .to_frame('para_str').sort_index()\n",
    "PARAS.index.names = OHCO[:2]\n",
    "\n",
    "PARAS['para_str'] = PARAS['para_str'].str.replace(r'\\n', ' ', regex=True)\n",
    "PARAS['para_str'] = PARAS['para_str'].str.strip()\n",
    "PARAS = PARAS[~PARAS['para_str'].str.match(r'^\\s*$')] # Remove empty paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split paragraphs into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_pat = r'[.?!;:\"]+'\n",
    "sent_pat = r'[.?!;:]+'\n",
    "SENTS = PARAS['para_str'].str.split(sent_pat, expand=True).stack()\\\n",
    "    .to_frame('sent_str')\n",
    "SENTS.index.names = OHCO[:3]\n",
    "\n",
    "SENTS = SENTS[~SENTS['sent_str'].str.match(r'^\\s*$')] # Remove empty paragraphs\n",
    "SENTS.sent_str = SENTS.sent_str.str.strip() # CRUCIAL TO REMOVE BLANK TOKENS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split sentences into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pat = r\"[\\s',-]+\"\n",
    "TOKENS = SENTS['sent_str'].str.split(token_pat, expand=True).stack()\\\n",
    "    .to_frame('token_str')\n",
    "\n",
    "TOKENS.index.names = OHCO[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine _Persuasion_ and _Sense and Sensibility_\n",
    "\n",
    "### <mark>Question 1.</mark>\n",
    "\n",
    "The number of raw tokens in the combined data frame is <b>207896</b> as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>token_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_num</th>\n",
       "      <th>chap_num</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dashwood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">24</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">13</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>6</th>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Persuasion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207896 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                token_str\n",
       "book_num chap_num para_num sent_num token_num            \n",
       "1        1        0        0        0                 The\n",
       "                                    1              family\n",
       "                                    2                  of\n",
       "                                    3            Dashwood\n",
       "                                    4                 had\n",
       "...                                                   ...\n",
       "2        24       13       0        6                  of\n",
       "                                    7          Persuasion\n",
       "                                    8                  by\n",
       "                                    9                Jane\n",
       "                                    10             Austen\n",
       "\n",
       "[207896 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file  = f\"{data_home}/austen-persuasion.csv\" \n",
    "temp_df = pd.read_csv(csv_file)\n",
    "temp_df = temp_df[['chap_num', 'para_num','sent_num','token_num', 'token_str']]\n",
    "OHCO = ['book_num','chap_num', 'para_num', 'sent_num', 'token_num']\n",
    "temp_df.insert(0, \"book_num\", [2]*temp_df.shape[0], True)\n",
    "def gather(ohco_level):\n",
    "    global TOKENS\n",
    "    level_name = OHCO[ohco_level-1].split('_')[0]\n",
    "    df = temp_df.groupby(OHCO[:ohco_level])\\\n",
    "        .token_str.apply(lambda x: x.str.cat(sep=' '))\\\n",
    "        .to_frame(f\"{level_name}_str\")\n",
    "    return df\n",
    "# gather(1)\n",
    "# gather(2)\n",
    "temp_df = gather(5)\n",
    "\n",
    "TOKENS.insert(0, \"book_num\", [1]*TOKENS.shape[0], True)\n",
    "def gather(ohco_level):\n",
    "    global TOKENS\n",
    "    level_name = OHCO[ohco_level-1].split('_')[0]\n",
    "    df = TOKENS.groupby(OHCO[:ohco_level])\\\n",
    "        .token_str.apply(lambda x: x.str.cat(sep=' '))\\\n",
    "        .to_frame(f\"{level_name}_str\")\n",
    "    return df\n",
    "TOKENS = gather(5)\n",
    "\n",
    "merged = pd.concat([TOKENS, temp_df])\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Question 2.</mark>\n",
    "\n",
    "The number of distinct terms in the combined data frame is <b>8239</b> as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>7435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to</td>\n",
       "      <td>6923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>6290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>6146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>her</td>\n",
       "      <td>3747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8234</th>\n",
       "      <td>unconquerable</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8235</th>\n",
       "      <td>outgrown</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8236</th>\n",
       "      <td>prosperously</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8237</th>\n",
       "      <td>nominal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8238</th>\n",
       "      <td>finis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8239 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              term_str     n\n",
       "term_id                     \n",
       "0                  the  7435\n",
       "1                   to  6923\n",
       "2                  and  6290\n",
       "3                   of  6146\n",
       "4                  her  3747\n",
       "...                ...   ...\n",
       "8234     unconquerable     1\n",
       "8235          outgrown     1\n",
       "8236      prosperously     1\n",
       "8237           nominal     1\n",
       "8238             finis     1\n",
       "\n",
       "[8239 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged['term_str'] = merged.token_str.replace(r'[\\W_]+', '', regex=True).str.lower()\n",
    "VOCAB = merged.term_str.value_counts().to_frame('n').reset_index().rename(columns={'index':'term_str'})\n",
    "VOCAB.index.name = 'term_id'\n",
    "VOCAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Question 3.</mark>\n",
    "\n",
    "<b>520</b> more terms.\n",
    "_Sense and Sensibility_ has 6280 terms. _Persuasion_ has 5760 terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6280 5760\n"
     ]
    }
   ],
   "source": [
    "TOKENS['term_str'] = TOKENS.token_str.replace(r'[\\W_]+', '', regex=True).str.lower()\n",
    "VOCAB1 = TOKENS.term_str.value_counts().to_frame('n').reset_index().rename(columns={'index':'term_str'})\n",
    "VOCAB1.index.name = 'term_id'\n",
    "\n",
    "sense_len = len(VOCAB1)\n",
    "\n",
    "temp_df['term_str'] = temp_df.token_str.replace(r'[\\W_]+', '', regex=True).str.lower()\n",
    "VOCAB2 = temp_df.term_str.value_counts().to_frame('n').reset_index().rename(columns={'index':'term_str'})\n",
    "VOCAB2.index.name = 'term_id'\n",
    "\n",
    "per_len = len(VOCAB2)\n",
    "\n",
    "print(sense_len, per_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Question 4.</mark>\n",
    "\n",
    "The average number of tokens per chapter in the corpus is <b>2809</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2809.4054054054054"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_tokens_per_chapter = merged.groupby(['book_num', 'chap_num']).size().mean()\n",
    "average_tokens_per_chapter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Question 5.</mark>\n",
    "\n",
    "The average number of tokens per paragraph in the corpus is <b>74</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.74813763746009"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_tokens_per_paragraph = merged.groupby(['book_num', 'chap_num', 'para_num']).size().mean()\n",
    "average_tokens_per_paragraph "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

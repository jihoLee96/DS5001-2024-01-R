{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M04 Homework\n",
    "\n",
    "```yaml\n",
    "Course:   DS 5001\n",
    "Module:   04 Lab\n",
    "Topic:    NLP and the Pipeline\n",
    "Author:   JiHo Lee (qxz6hb)\n",
    "Date:     10 February 2023\n",
    "```\n",
    "\n",
    "### Question 1. What regular expression did you use to chunk _Middlemarch_ into chapters?\n",
    "\n",
    "> The regular expression used to chunk _Middlemarch_ into chapters is ```^\\s*CHAPTER\\s+[IVXLCM]+\\.\\s*$```\n",
    "\n",
    "### Question 2. What is the title of the book that has the most tokens? \n",
    "\n",
    "> MIDDLEMARCH\n",
    "\n",
    "_Middlemarch_ has the most number of tokens which is 317305.\n",
    "\n",
    "### Question 3. How many chapter level chunks are there in this novel?\n",
    "\n",
    "> _Middlemarch_: 86 \n",
    "\n",
    "The number of chapters in _Middlemarch_ is 86.\n",
    "\n",
    "_Adam bede_ and _The mill on the floss_ has 55, 58 chapters respectively.\n",
    "\n",
    "### Question 4. Among the three stemming algorithms -- Porter, Lancaster, and Snowball --  which is the most aggressive, in terms of the number of words associated with each stem?\n",
    "\n",
    "> Lancaster\n",
    "\n",
    "\n",
    "\n",
    "### Question 5. Using the most aggressive stemmer from the previous question, what is the stem with the most associated terms?\n",
    "\n",
    "> The stem with the most associated terms is ***'cont'*** with 34 times.\n",
    "\n",
    "\n",
    "\n",
    "#### <mark>The code related to questions and answers with explanation is described in each corresponding section below.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwrVU8kZDykb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import nltk\n",
    "import plotly_express as px\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../../../env.ini\")\n",
    "data_home = config['DEFAULT']['data_home']\n",
    "output_dir = config['DEFAULT']['output_dir']\n",
    "local_lib = config['DEFAULT']['local_lib']\n",
    "\n",
    "data_home = data_home.replace('/', '\\\\')\n",
    "output_dir = output_dir.replace('/', '\\\\')\n",
    "local_lib = local_lib.replace('/', '\\\\')\n",
    "\n",
    "source_files = f'{data_home}/gutenberg/eliot-set'\n",
    "data_prefix = 'eliot'\n",
    "\n",
    "OHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']\n",
    "\n",
    "import sys\n",
    "sys.path.append(local_lib)\n",
    "# print(local_lib)\n",
    "\n",
    "from textparser import TextParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_pats = [\n",
    "    r\"\\*\\*\\*\\s*START OF\",\n",
    "    r\"\\*\\*\\*\\s*END OF\"\n",
    "]\n",
    "\n",
    "# All are 'chap'and 'm'\n",
    "roman = '[IVXLCM]+'\n",
    "caps = \"[A-Z';, -]+\"\n",
    "ohco_pat_list = [\n",
    "    (145, rf\"^\\s*CHAPTER\\s+{roman}\\.\\s*$\"),\n",
    "#     (507, rf\"^\\s*Chapter\\s+{roman}\\s\"),\n",
    "    (507, rf\"^\\s*Chapter\\s+{roman}\\b$\"),\n",
    "    (6688, rf\"^\\s*Chapter\\s+{roman}\\.\\s*$\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register\n",
    "\n",
    "We get each file and add to a library `LIB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file_list = sorted(glob(f\"{source_files}/*.*\"))\n",
    "\n",
    "book_data = []\n",
    "for source_file_path in source_file_list:\n",
    "    source_file_path = source_file_path.replace('/', '\\\\')\n",
    "#     print(source_file_path)\n",
    "    book_id = int(source_file_path.split('-')[-1].split('.')[0].replace('pg',''))\n",
    "    book_title = source_file_path.split('\\\\')[-1].split('-')[0].replace('_', ' ')\n",
    "    book_data.append((book_id, source_file_path, book_title))\n",
    "    \n",
    "LIB = pd.DataFrame(book_data, columns=['book_id','source_file_path','raw_title'])\\\n",
    "    .set_index('book_id').sort_index()\n",
    "\n",
    "try:\n",
    "    LIB['author'] = LIB.raw_title.apply(lambda x: ', '.join(x.split()[:2]))\n",
    "    LIB['title'] = LIB.raw_title.apply(lambda x: ' '.join(x.split()[2:]))\n",
    "    LIB = LIB.drop('raw_title', axis=1)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "LIB['chap_regex'] = LIB.index.map(pd.Series({x[0]:x[1] for x in ohco_pat_list}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Question 1.</mark>\n",
    "\n",
    "The regular expression used to chunk _Middlemarch_ into chapters is ```^\\s*CHAPTER\\s+[IVXLCM]+\\.\\s*$```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file_path</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>chap_regex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...</td>\n",
       "      <td>ELIOT, GEORGE</td>\n",
       "      <td>MIDDLEMARCH</td>\n",
       "      <td>^\\s*CHAPTER\\s+[IVXLCM]+\\.\\s*$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...</td>\n",
       "      <td>ELIOT, GEORGE</td>\n",
       "      <td>ADAM BEDE</td>\n",
       "      <td>^\\s*Chapter\\s+[IVXLCM]+\\b$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6688</th>\n",
       "      <td>C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...</td>\n",
       "      <td>ELIOT, GEORGE</td>\n",
       "      <td>THE MILL ON THE FLOSS</td>\n",
       "      <td>^\\s*Chapter\\s+[IVXLCM]+\\.\\s*$</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source_file_path         author  \\\n",
       "book_id                                                                     \n",
       "145      C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...  ELIOT, GEORGE   \n",
       "507      C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...  ELIOT, GEORGE   \n",
       "6688     C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...  ELIOT, GEORGE   \n",
       "\n",
       "                         title                     chap_regex  \n",
       "book_id                                                        \n",
       "145                MIDDLEMARCH  ^\\s*CHAPTER\\s+[IVXLCM]+\\.\\s*$  \n",
       "507                  ADAM BEDE     ^\\s*Chapter\\s+[IVXLCM]+\\b$  \n",
       "6688     THE MILL ON THE FLOSS  ^\\s*Chapter\\s+[IVXLCM]+\\.\\s*$  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Corpus\n",
    "\n",
    "We tokenize each book and add each `TOKENS` table to a list to be concatenated into a single `CORPUS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_collection(LIB):\n",
    "\n",
    "    clip_pats = [\n",
    "        r\"\\*\\*\\*\\s*START OF\",\n",
    "        r\"\\*\\*\\*\\s*END OF\"\n",
    "    ]\n",
    "\n",
    "    books = []\n",
    "    for book_id in LIB.index:\n",
    "\n",
    "        # Announce\n",
    "        print(\"Tokenizing\", book_id, LIB.loc[book_id].title)\n",
    "\n",
    "        # Define vars\n",
    "        chap_regex = LIB.loc[book_id].chap_regex\n",
    "        ohco_pats = [('chap', chap_regex, 'm')]\n",
    "        src_file_path = LIB.loc[book_id].source_file_path\n",
    "\n",
    "        # Create object\n",
    "        text = TextParser(src_file_path, ohco_pats=ohco_pats, clip_pats=clip_pats, use_nltk=True)\n",
    "\n",
    "        # Define parameters\n",
    "        text.verbose = True\n",
    "        text.strip_hyphens = True\n",
    "        text.strip_whitespace = True\n",
    "\n",
    "        # Parse\n",
    "        text.import_source().parse_tokens();\n",
    "\n",
    "        # Name things\n",
    "        text.TOKENS['book_id'] = book_id\n",
    "        text.TOKENS = text.TOKENS.reset_index().set_index(['book_id'] + text.OHCO)\n",
    "\n",
    "        # Add to list\n",
    "        books.append(text.TOKENS)\n",
    "        \n",
    "    # Combine into a single dataframe\n",
    "    CORPUS = pd.concat(books).sort_index()\n",
    "\n",
    "    # Clean up\n",
    "    del(books)\n",
    "    del(text)\n",
    "        \n",
    "    print(\"Done\")\n",
    "        \n",
    "    return CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 145 MIDDLEMARCH\n",
      "Importing  C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\eliot-set\\ELIOT_GEORGE_MIDDLEMARCH-pg145.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*CHAPTER\\s+[IVXLCM]+\\.\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 507 ADAM BEDE\n",
      "Importing  C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\eliot-set\\ELIOT_GEORGE_ADAM_BEDE-pg507.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*Chapter\\s+[IVXLCM]+\\b$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6688 THE MILL ON THE FLOSS\n",
      "Importing  C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\eliot-set\\ELIOT_GEORGE_THE_MILL_ON_THE_FLOSS-pg6688.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*Chapter\\s+[IVXLCM]+\\.\\s*$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "CORPUS = tokenize_collection(LIB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract some features for `LIB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB['book_len'] = CORPUS.groupby('book_id').term_str.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Question 2.</mark>\n",
    "\n",
    "_Middlemarch_ has the most number of tokens which is 317305 as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file_path</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>chap_regex</th>\n",
       "      <th>book_len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6688</th>\n",
       "      <td>C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...</td>\n",
       "      <td>ELIOT, GEORGE</td>\n",
       "      <td>THE MILL ON THE FLOSS</td>\n",
       "      <td>^\\s*Chapter\\s+[IVXLCM]+\\.\\s*$</td>\n",
       "      <td>207461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...</td>\n",
       "      <td>ELIOT, GEORGE</td>\n",
       "      <td>ADAM BEDE</td>\n",
       "      <td>^\\s*Chapter\\s+[IVXLCM]+\\b$</td>\n",
       "      <td>215404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...</td>\n",
       "      <td>ELIOT, GEORGE</td>\n",
       "      <td>MIDDLEMARCH</td>\n",
       "      <td>^\\s*CHAPTER\\s+[IVXLCM]+\\.\\s*$</td>\n",
       "      <td>317305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source_file_path         author  \\\n",
       "book_id                                                                     \n",
       "6688     C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...  ELIOT, GEORGE   \n",
       "507      C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...  ELIOT, GEORGE   \n",
       "145      C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...  ELIOT, GEORGE   \n",
       "\n",
       "                         title                     chap_regex  book_len  \n",
       "book_id                                                                  \n",
       "6688     THE MILL ON THE FLOSS  ^\\s*Chapter\\s+[IVXLCM]+\\.\\s*$    207461  \n",
       "507                  ADAM BEDE     ^\\s*Chapter\\s+[IVXLCM]+\\b$    215404  \n",
       "145                MIDDLEMARCH  ^\\s*CHAPTER\\s+[IVXLCM]+\\.\\s*$    317305  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB.sort_values('book_len')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Quesion 3.</mark>\n",
    "\n",
    "The number of chapters in _Middlemarch_ is 86.\n",
    "\n",
    "_Adam bede_ and _The mill on the floss_ has 55, 58 chapters respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file_path</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>chap_regex</th>\n",
       "      <th>book_len</th>\n",
       "      <th>n_chaps</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...</td>\n",
       "      <td>ELIOT, GEORGE</td>\n",
       "      <td>MIDDLEMARCH</td>\n",
       "      <td>^\\s*CHAPTER\\s+[IVXLCM]+\\.\\s*$</td>\n",
       "      <td>317305</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...</td>\n",
       "      <td>ELIOT, GEORGE</td>\n",
       "      <td>ADAM BEDE</td>\n",
       "      <td>^\\s*Chapter\\s+[IVXLCM]+\\b$</td>\n",
       "      <td>215404</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6688</th>\n",
       "      <td>C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...</td>\n",
       "      <td>ELIOT, GEORGE</td>\n",
       "      <td>THE MILL ON THE FLOSS</td>\n",
       "      <td>^\\s*Chapter\\s+[IVXLCM]+\\.\\s*$</td>\n",
       "      <td>207461</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source_file_path         author  \\\n",
       "book_id                                                                     \n",
       "145      C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...  ELIOT, GEORGE   \n",
       "507      C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...  ELIOT, GEORGE   \n",
       "6688     C:\\DS5001\\DS5001_2024_01_R\\..\\data\\gutenberg\\e...  ELIOT, GEORGE   \n",
       "\n",
       "                         title                     chap_regex  book_len  \\\n",
       "book_id                                                                   \n",
       "145                MIDDLEMARCH  ^\\s*CHAPTER\\s+[IVXLCM]+\\.\\s*$    317305   \n",
       "507                  ADAM BEDE     ^\\s*Chapter\\s+[IVXLCM]+\\b$    215404   \n",
       "6688     THE MILL ON THE FLOSS  ^\\s*Chapter\\s+[IVXLCM]+\\.\\s*$    207461   \n",
       "\n",
       "         n_chaps  \n",
       "book_id           \n",
       "145           86  \n",
       "507           55  \n",
       "6688          58  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB['n_chaps'] = CORPUS.reset_index()[['book_id','chap_id']]\\\n",
    "    .drop_duplicates()\\\n",
    "    .groupby('book_id').chap_id.count()\n",
    "\n",
    "LIB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exract VOCAB\n",
    "\n",
    "Extract a vocabulary from the CORPUS as a whole\n",
    "\n",
    "### Handle Anomalies\n",
    "\n",
    "NLTK's POS tagger is not perfect -- note the classification of punctuation as nouns, verbs, etc. We remove these from our corups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# handle anomalies\n",
    "CORPUS = CORPUS[CORPUS.term_str != '']\n",
    "CORPUS['pos_group'] = CORPUS.pos.str[:2]\n",
    "\n",
    "VOCAB = CORPUS.term_str.value_counts().to_frame('n').sort_index()\n",
    "VOCAB.index.name = 'term_str'\n",
    "VOCAB['n_chars'] = VOCAB.index.str.len()\n",
    "VOCAB['p'] = VOCAB.n / VOCAB.n.sum()\n",
    "VOCAB['i'] = -np.log2(VOCAB.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bDSH9L2TXGzH",
    "tags": []
   },
   "source": [
    "### Add Stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mE_YGklKXSYn"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>p</th>\n",
       "      <th>i</th>\n",
       "      <th>stem_porter</th>\n",
       "      <th>stem_snowball</th>\n",
       "      <th>stem_lancaster</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abjectly</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.497458</td>\n",
       "      <td>abjectli</td>\n",
       "      <td>abject</td>\n",
       "      <td>abject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abruptly</th>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>15.497458</td>\n",
       "      <td>abruptli</td>\n",
       "      <td>abrupt</td>\n",
       "      <td>abrupt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abstractedly</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>17.912496</td>\n",
       "      <td>abstractedli</td>\n",
       "      <td>abstract</td>\n",
       "      <td>abstract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abundantly</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>17.497458</td>\n",
       "      <td>abundantli</td>\n",
       "      <td>abund</td>\n",
       "      <td>abund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accordingly</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>16.038027</td>\n",
       "      <td>accordingli</td>\n",
       "      <td>accord</td>\n",
       "      <td>accord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeswellyou</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.497458</td>\n",
       "      <td>yeswelly</td>\n",
       "      <td>yeswellyou</td>\n",
       "      <td>yeswellyou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yous</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>17.912496</td>\n",
       "      <td>you</td>\n",
       "      <td>yous</td>\n",
       "      <td>yo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zealous</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>16.175530</td>\n",
       "      <td>zealou</td>\n",
       "      <td>zealous</td>\n",
       "      <td>zeal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>æschylus</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>18.497458</td>\n",
       "      <td>æschylu</td>\n",
       "      <td>æschylus</td>\n",
       "      <td>æschylus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>œdipus</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>18.497458</td>\n",
       "      <td>œdipu</td>\n",
       "      <td>œdipus</td>\n",
       "      <td>œdip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>655 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               n  n_chars         p          i   stem_porter stem_snowball  \\\n",
       "term_str                                                                     \n",
       "abjectly       1        8  0.000001  19.497458      abjectli        abject   \n",
       "abruptly      16        8  0.000022  15.497458      abruptli        abrupt   \n",
       "abstractedly   3       12  0.000004  17.912496  abstractedli      abstract   \n",
       "abundantly     4       10  0.000005  17.497458    abundantli         abund   \n",
       "accordingly   11       11  0.000015  16.038027   accordingli        accord   \n",
       "...           ..      ...       ...        ...           ...           ...   \n",
       "yeswellyou     1       10  0.000001  19.497458      yeswelly    yeswellyou   \n",
       "yous           3        4  0.000004  17.912496           you          yous   \n",
       "zealous       10        7  0.000014  16.175530        zealou       zealous   \n",
       "æschylus       2        8  0.000003  18.497458       æschylu      æschylus   \n",
       "œdipus         2        6  0.000003  18.497458         œdipu        œdipus   \n",
       "\n",
       "             stem_lancaster  \n",
       "term_str                     \n",
       "abjectly             abject  \n",
       "abruptly             abrupt  \n",
       "abstractedly       abstract  \n",
       "abundantly            abund  \n",
       "accordingly          accord  \n",
       "...                     ...  \n",
       "yeswellyou       yeswellyou  \n",
       "yous                     yo  \n",
       "zealous                zeal  \n",
       "æschylus           æschylus  \n",
       "œdipus                 œdip  \n",
       "\n",
       "[655 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer1 = PorterStemmer()\n",
    "VOCAB['stem_porter'] = VOCAB.apply(lambda x: stemmer1.stem(x.name), 1)\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer2 = SnowballStemmer(\"english\")\n",
    "VOCAB['stem_snowball'] = VOCAB.apply(lambda x: stemmer2.stem(x.name), 1)\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer3 = LancasterStemmer()\n",
    "VOCAB['stem_lancaster'] = VOCAB.apply(lambda x: stemmer3.stem(x.name), 1)\n",
    "\n",
    "VOCAB[VOCAB.stem_porter != VOCAB.stem_snowball]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Question 4.</mark>\n",
    "\n",
    "_Lancaster_ is the most aggressive stemming algorithm.\n",
    "\n",
    "Each _Porter, Snawball, Lancaster_ algorithm has 17540, 17203, 14612 unique number of stems.\n",
    "And, the number of changed words associated with each stem is 16823, 16617, 19401 respectively.\n",
    "Also, the average number of associated words per each stem is 1.50, 1.53, 1.80 respectively.\n",
    "Therefore, _Lancaster_ aggressively changed the most number of words with its associated stem which is the least unique number of stems among three algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stemmer</th>\n",
       "      <th>Unique_Stems</th>\n",
       "      <th># of changed term_str</th>\n",
       "      <th>Average of # of associated words with each stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Porter</td>\n",
       "      <td>17540</td>\n",
       "      <td>16823</td>\n",
       "      <td>1.501539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Snowball</td>\n",
       "      <td>17203</td>\n",
       "      <td>16617</td>\n",
       "      <td>1.530954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lancaster</td>\n",
       "      <td>14612</td>\n",
       "      <td>19401</td>\n",
       "      <td>1.802423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Stemmer  Unique_Stems  # of changed term_str  \\\n",
       "0     Porter         17540                  16823   \n",
       "1   Snowball         17203                  16617   \n",
       "2  Lancaster         14612                  19401   \n",
       "\n",
       "   Average of # of associated words with each stem  \n",
       "0                                         1.501539  \n",
       "1                                         1.530954  \n",
       "2                                         1.802423  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_porter = VOCAB['stem_porter'].nunique()\n",
    "unique_snowball = VOCAB['stem_snowball'].nunique()\n",
    "unique_lancaster = VOCAB['stem_lancaster'].nunique()\n",
    "\n",
    "comp1 = VOCAB[VOCAB.index != VOCAB.stem_porter]\n",
    "comp2 = VOCAB[VOCAB.index != VOCAB.stem_snowball]\n",
    "comp3 = VOCAB[VOCAB.index != VOCAB.stem_lancaster]\n",
    "\n",
    "n1 = len(comp1)\n",
    "n2 = len(comp2)\n",
    "n3 = len(comp3)\n",
    "\n",
    "porter_grouped = VOCAB.groupby('stem_porter').size().reset_index(name='n_terms')\n",
    "avg1 = porter_grouped['n_terms'].mean()\n",
    "snawball_grouped = VOCAB.groupby('stem_snowball').size().reset_index(name='n_terms')\n",
    "avg2 = snawball_grouped['n_terms'].mean()\n",
    "lancaster_grouped = VOCAB.groupby('stem_lancaster').size().reset_index(name='n_terms')\n",
    "avg3 = lancaster_grouped['n_terms'].mean()\n",
    "\n",
    "comparison = {\n",
    "    'Stemmer': ['Porter', 'Snowball', 'Lancaster'],\n",
    "    'Unique_Stems': [unique_porter, unique_snowball, unique_lancaster],\n",
    "    '# of changed term_str': [n1, n2, n3],\n",
    "    'Average of # of associated words with each stem' : [avg1, avg2, avg3]\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame(comparison)\n",
    "\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Question 5.</mark>\n",
    "\n",
    "The stem with the most associated terms is ***'cont'*** with 34 times as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cont :  34\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stem_lancaster</th>\n",
       "      <th>n_terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>cont</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     stem_lancaster  n_terms\n",
       "2481           cont       34"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lancaster is the most aggressive algorithm\n",
    "lancaster_grouped = VOCAB.groupby('stem_lancaster').size().reset_index(name='n_terms')\n",
    "tmp = lancaster_grouped.loc[lancaster_grouped['n_terms'].idxmax()]\n",
    "\n",
    "stem = tmp['stem_lancaster']\n",
    "cnt = tmp['n_terms']\n",
    "\n",
    "print(stem, ': ', cnt)\n",
    "lancaster_grouped[lancaster_grouped['stem_lancaster']=='cont']"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS5559_Annotations.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
